{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35fe8e2-ed47-4e84-9b1d-e068893d29de",
   "metadata": {},
   "source": [
    "### Convert ECB Model Guide to machine-readable format\n",
    "\n",
    "This code aims to process and clean a document from the European Central Bank (ECB) about internal models. The code provides a structured approach to extracting, cleaning, and processing text from the document.\n",
    "\n",
    "#### Overview\n",
    "\n",
    "1. **Rules for Text Formatting**: There are 4 primary rules that dictate how the text should be formatted based on specific patterns.\n",
    "2. **Text Extraction**: The code imports necessary libraries and sets up the environment. It defines functions to extract text from a PDF using two different methods (`pypdf` and `pdfminer`).\n",
    "3. **Text Cleaning**: It cleans the text, particularly removing headers and footers and joining separated sentences.\n",
    "4. **Text Processing**: The code processes the text to follow the formatting rules mentioned above. It identifies titles and subtitles and formats them appropriately.\n",
    "5. **Data Extraction**: The formatted text is then broken down into structured data with levels of headings and body content.\n",
    "6. **Text Cleaning with GPT-3**: The code utilizes OpenAI's GPT-3 to perform additional spell checks on the text.\n",
    "7. **Embedding**: The code generates embeddings for the cleaned text using OpenAI's embeddings utility.\n",
    "\n",
    "#### Detailed Breakdown\n",
    "\n",
    "- **Rules for Text Formatting**:\n",
    "    - Rule 1: Format lines like \"x.y [text]\" with \"###\".\n",
    "    - Rule 2: Format lines like \"x [text]\" followed by lines from Rule 1 with \"##\".\n",
    "    - Rule 3: Format lines like \"x.x.x [text]\" with \"####\".\n",
    "    - Rule 4: Format lines like \"x. [text]\" with a preceding newline.\n",
    "\n",
    "- **Text Extraction**:\n",
    "    - Two methods to extract text from a PDF (`extract_pdf_pypdf` and `extract_pdf_pdfminer`).\n",
    "    - Save extracted text to `.txt` files.\n",
    "\n",
    "- **Text Cleaning**:\n",
    "    - Count words in the extracted files.\n",
    "    - Remove unwanted headers and footers.\n",
    "    - Process text to concatenate separated sentences.\n",
    "\n",
    "- **Text Processing**:\n",
    "    - Apply the formatting rules to the text. For example, if a line matches the format \"x.y [text]\", it will be prefixed with \"###\".\n",
    "\n",
    "- **Data Extraction**:\n",
    "    - The processed text is broken down into a DataFrame where each row represents a paragraph or section of the document. The DataFrame has columns for each level of heading (from main heading to sub-sub-heading) and a column for the body content.\n",
    "    - Some columns are added to the DataFrame for further analysis, such as counting words and checking specific patterns.\n",
    "\n",
    "- **Text Cleaning with GPT-3**:\n",
    "    - The text is sent to GPT-3 for spell-checking. This is particularly useful for words that may have been incorrectly split during the conversion from PDF to text.\n",
    "    - The corrected text replaces the original in the DataFrame.\n",
    "\n",
    "- **Embedding**:\n",
    "    - The cleaned text is then processed to generate embeddings. This is useful for future applications such as similarity checks, clustering, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fe2d33-d42e-4989-a959-91eebc0ad347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "from pdfminer.high_level import extract_text\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import time\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import get_embedding\n",
    "\n",
    "# Settings\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721c806-cb7c-4ebb-9b85-d48fe9759f31",
   "metadata": {},
   "source": [
    "#### A. Import the ECB Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83fcf7-1fa1-4680-abe2-4f0e31bef876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract document using different readers\n",
    "def extract_pdf_pypdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        full_text += page.extract_text()\n",
    "    return full_text\n",
    "\n",
    "def extract_pdf_pdfminer(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def save_to_txt(text, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "# Extract and save files\n",
    "pdf_path = \"ssm.pubcon230622_guide.en.pdf\"\n",
    "text = extract_pdf_pypdf(pdf_path)\n",
    "text_pdfminer = extract_pdf_pdfminer(pdf_path)\n",
    "\n",
    "save_to_txt(text, 'trim_guide_pypdf.txt')\n",
    "save_to_txt(text_pdfminer, 'trim_guide_pdfminer.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b2427-fec2-4672-9e97-bf33724b4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A. Import the ECB Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68716b-7622-4c4a-86da-0e41714488a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Use regex to tokenize the text and find words with more than 3 letters\n",
    "    words = re.findall(r'\\b\\w{4,}\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "# Paths to the three text files\n",
    "file1 = 'trim_guide_pypdf.txt'\n",
    "file2 = 'trim_guide_pdfminer.txt'\n",
    "file3 = 'ssm.pubcon230622_guide.en.txt'\n",
    "\n",
    "# Get word counts for each file\n",
    "count1 = count_words(file1)\n",
    "count2 = count_words(file2)\n",
    "count3 = count_words(file3)\n",
    "\n",
    "# Print comparison\n",
    "print(f\"Words with more than 3 letters in {file1}: {count1}\")\n",
    "print(f\"Words with more than 3 letters in {file2}: {count2}\")\n",
    "print(f\"Words with more than 3 letters in {file3}: {count3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e7cb2-fe8d-4584-a8af-63f3903fdcfa",
   "metadata": {},
   "source": [
    "#### B. Basic cleansing activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e193dd-8e34-4c5b-9946-b265c1da383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove header and footer\n",
    "def remove_ecb_guide_from_file(filename):\n",
    "    # Pattern to match the unwanted lines\n",
    "    pattern = r'^ECB guide to internal models\\s*–.*?\\d+'\n",
    "    \n",
    "    # Read the file line by line and clean each line\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        cleaned_lines = [re.sub(pattern, '', line) for line in lines]\n",
    "    \n",
    "    # Write the cleaned lines back to the file\n",
    "    with open('trim_guide_pypdf_no_footer.txt', 'w', encoding='utf-8') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "\n",
    "# Call the function\n",
    "filename = \"trim_guide_pypdf.txt\"\n",
    "remove_ecb_guide_from_file(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df6d86-5a75-49ea-9b8c-f478c5e32c32",
   "metadata": {},
   "source": [
    "#### C. Create inventory of headings and convert the doc to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d97e3-a961-4045-b332-54975f0e980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_to_list(version_str):\n",
    "    return list(map(int, version_str.split('.')))\n",
    "\n",
    "def increment_by_one(v1, v2):\n",
    "    if (v1 == \"\") | (v2 == \"\"):\n",
    "        return True\n",
    "\n",
    "    v1_list = version_to_list(v1)\n",
    "    v2_list = version_to_list(v2)\n",
    "    \n",
    "    if len(v1_list) != len(v2_list):\n",
    "        return False\n",
    "    \n",
    "    diff_count = 0\n",
    "    for a, b in zip(v1_list, v2_list):\n",
    "        if b < a:\n",
    "            return False\n",
    "        if b - a > 1:\n",
    "            return False\n",
    "        if b - a == 1:\n",
    "            diff_count += 1\n",
    "        if b - a == 0 and diff_count > 0:\n",
    "            return False\n",
    "            \n",
    "    return diff_count == 1\n",
    "\n",
    "def count_parts(s):\n",
    "    return len(s.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2d19f-02f7-435b-a78b-4e6369584d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Add a sentinel line at the end to make sure we process the last line correctly\n",
    "    lines.append('Sentinel line')\n",
    "\n",
    "    last_seen_main = \"\"\n",
    "    last_seen_sub = \"\"\n",
    "    last_seen_sub_sub = \"\"\n",
    "    footnote = \"0\"\n",
    "    \n",
    "    num_par = \"\"\n",
    "\n",
    "    processed_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines) - 1:\n",
    "        # print(i, len(lines))\n",
    "        line = lines[i].strip()\n",
    "        next_line = lines[i+1].strip()\n",
    "\n",
    "\n",
    "        k = re.match(r'^(\\d+(\\.\\d+){0,2}) (\\s*[A-Z])', line)\n",
    "        if k:\n",
    "            num_par = k.groups()[0]\n",
    "            # print(i, num_par)\n",
    "        \n",
    "        # New Rule: Appending # to a sentence if the next non-empty line starts with 1 and matches the criteria of Rule 2\n",
    "        j = i + 1\n",
    "        while j < len(lines) and not lines[j].strip():  # Skip empty lines\n",
    "            j += 1\n",
    "        \n",
    "        if j < len(lines) and lines[j].startswith('1') and re.match(r'^\\d+ [A-Z]', lines[j]) and re.match(r'^[A-Z]', line):\n",
    "            processed_lines.append('# ' + line + '\\n')\n",
    "        # Rule 1\n",
    "        elif (re.match(r'^\\d+\\.\\d+ [A-Z]', line) is not None) & increment_by_one(last_seen_sub, num_par):\n",
    "\n",
    "            if next_line[0].islower():\n",
    "                line = line + \" \" + next_line\n",
    "                i = i + 1\n",
    "            \n",
    "            processed_lines.append('\\n')\n",
    "            processed_lines.append('### ' + line + '\\n')\n",
    "            processed_lines.append('\\n')\n",
    "            last_seen_sub = num_par\n",
    "\n",
    "        elif (re.match(r'^(\\d+\\.\\d+\\.\\d+) {1,3}[A-Z]', line) is not None):\n",
    "            if next_line[0].islower():\n",
    "                line = line + \" \" + next_line\n",
    "                i = i + 1\n",
    "                \n",
    "            processed_lines.append('\\n')\n",
    "            processed_lines.append('#### ' + line + '\\n')\n",
    "            processed_lines.append('\\n')\n",
    "            last_seen_sub_sub = num_par\n",
    "        # Rule 2 todo\n",
    "        elif re.match(r'^\\d+ [A-Z]', line) and (re.match(r'^\\d+\\.\\d+ [A-Z]', lines[i + 1]) or re.match(r'^\\d+\\. [A-Z]', lines[i + 1])):\n",
    "\n",
    "            if next_line[0].islower():\n",
    "                line = line + \" \" + next_line\n",
    "                i = i + 1\n",
    "                \n",
    "            processed_lines.append('\\n')\n",
    "            processed_lines.append('## ' + line + '\\n')\n",
    "            processed_lines.append('\\n')\n",
    "            last_seen_main = num_par\n",
    "            last_seen_sub = \"\"\n",
    "        # Rule 4\n",
    "        elif re.match(r'^\\d+\\. [A-Z]', line):\n",
    "            processed_lines.append('\\n')\n",
    "            processed_lines.append(line + '\\n')\n",
    "\n",
    "        elif (re.match(r'(^\\d+)\\s{2}([A-Z\"“]+)', line) is not None):\n",
    "            processed_lines.append('\\n')\n",
    "            processed_lines.append('FOOTNOTE ' + line + '\\n')\n",
    "        \n",
    "        else:\n",
    "            processed_lines.append(line + '\\n')\n",
    "            footnote = num_par\n",
    "        i += 1\n",
    "\n",
    "    # Write to the modified file\n",
    "    with open('trim_guide_pypdf_mod_v6.txt', 'w', encoding='utf-8') as f:\n",
    "        f.writelines(processed_lines)\n",
    "\n",
    "# Call the function\n",
    "filename = 'trim_guide_pypdf_no_footer.txt'\n",
    "process_text_file(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24554d-ec2f-4435-b2b0-3cf91507665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def starts_with_integer_ends_with_dot(s: str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the given string starts with an integer and ends with a dot, or if it starts with the word \"Table\", or if it ends with a floating point.\n",
    "    Args:\n",
    "    - s (str): Input string to check.\n",
    "    \n",
    "    Returns:\n",
    "    - int: 1 if conditions are met, otherwise 0.\n",
    "    \"\"\"\n",
    "    pattern_1 = r'^\\d+.*\\.$'\n",
    "    pattern_2 = r'^Table'\n",
    "    pattern_3 = r'^\\d+.*\\.\\d+$'\n",
    "    \n",
    "    if re.match(pattern_1, s) or re.match(pattern_2, s) or re.match(pattern_3, s):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def extract_data(text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract data from the given text based on various conditions and create a DataFrame.\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Extracted data in DataFrame format.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    data = []\n",
    "    \n",
    "    # Initialize labels\n",
    "    level_0_label, level_1_label, level_2_label, level_3_label, level_4_label, level_5_label = [None]*6\n",
    "    body_buffer = []\n",
    "    \n",
    "    for i in range(len(lines) - 1):\n",
    "        stripped_line = lines[i].strip()\n",
    "        stripped_line_2 = lines[i+1].strip()\n",
    "\n",
    "        # Detect headings based on our markers\n",
    "        if stripped_line.startswith(\"FOOTNOTE\"):\n",
    "            match = re.search(r\"FOOTNOTE\\s*(\\d+)\", stripped_line) \n",
    "            level_5_label = f'Footnote {match.group(1)}'\n",
    "            body_buffer.append(stripped_line)\n",
    "            \n",
    "        elif stripped_line.startswith(\"#####\"):\n",
    "            level_4_label = stripped_line.replace(\"#####\", \"\").strip()\n",
    "            level_5_label = None\n",
    "            \n",
    "        elif stripped_line.startswith(\"####\"):\n",
    "            level_3_label = stripped_line.replace(\"####\", \"\").strip()\n",
    "            level_4_label, level_5_label = None, None\n",
    "            \n",
    "        elif stripped_line.startswith(\"###\"):\n",
    "            level_2_label = stripped_line.replace(\"###\", \"\").strip()\n",
    "            level_3_label, level_4_label, level_5_label = [None]*3\n",
    "            \n",
    "        elif stripped_line.startswith(\"##\"):\n",
    "            level_1_label = stripped_line.replace(\"##\", \"\").strip()\n",
    "            level_2_label, level_3_label, level_4_label, level_5_label = [None]*4\n",
    "            \n",
    "        elif stripped_line.startswith(\"#\"):\n",
    "            level_0_label = stripped_line.replace(\"#\", \"\").strip()\n",
    "            level_1_label, level_2_label, level_3_label, level_4_label, level_5_label = [None]*5\n",
    "            \n",
    "        else:\n",
    "            body_buffer.append(stripped_line)\n",
    "\n",
    "        # Create body buffer\n",
    "\n",
    "        # Condition 1: Check if the line is empty and there's content in the buffer\n",
    "        is_empty_line_with_buffer = (not stripped_line) and body_buffer\n",
    "        \n",
    "        # Condition 2a: The next line is either empty or starts with a lowercase letter\n",
    "        next_line_starts_lower = len(stripped_line_2) == 0 or stripped_line_2[0].islower()\n",
    "        \n",
    "        # Condition 2b: The current line ends with a period but not with 'e.g.' or 'i.e.'\n",
    "        current_line_special_end = stripped_line.endswith('.') and not (stripped_line.endswith('e.g.') or stripped_line.endswith('i.e.'))\n",
    "        \n",
    "        # Combine the conditions\n",
    "        should_end_paragraph = is_empty_line_with_buffer or (next_line_starts_lower and current_line_special_end)\n",
    "        \n",
    "        # Use the final condition in the 'if' statement\n",
    "        if should_end_paragraph:\n",
    "            # Exclude blank lines from body buffer\n",
    "            cleaned_body_buffer = [line for line in body_buffer if line]\n",
    "            if cleaned_body_buffer:\n",
    "                data.append([level_0_label, level_1_label, level_2_label, level_3_label, level_4_label, level_5_label, ' '.join(body_buffer)])\n",
    "                level_5_label = None\n",
    "                        \n",
    "            body_buffer = []\n",
    "\n",
    "    # Add the last buffered body text if present\n",
    "    if body_buffer:\n",
    "        data.append([level_0_label, level_1_label, level_2_label, level_3_label, level_4_label, level_5_label, ' '.join(body_buffer)])\n",
    "        level_5_label = None\n",
    "\n",
    "    # Convert data to Pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Level_0_Label', 'Level_1_Label', 'Level_2_Label', 'Level_3_Label', 'Level_4_Label', 'Level_5_Label', 'Body'])\n",
    "\n",
    "    # Add new columns based on various conditions\n",
    "    df['num_of_words'] = df['Body'].apply(lambda x: sum(1 for word in x.split() if len(word) >= 3))\n",
    "    df['no_fault_detected'] = df['Body'].apply(starts_with_integer_ends_with_dot)\n",
    "    df['Body'] = df['Body'].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a7273-96aa-4cd7-9af2-c80da0f2b9d3",
   "metadata": {},
   "source": [
    "#### D. After-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85a117-dcd0-4b30-b556-874733c07fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans text columns based on specified patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the columns to clean.\n",
    "    - columns: List of column names to apply the cleaning on.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with cleaned columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.replace('  ', ' ', regex=True) \\\n",
    "                         .str.replace(' -', '-', regex=True) \\\n",
    "                         .str.replace('regulato ry', 'regulatory', regex=False) \\\n",
    "                         .str.replace('B anking', 'Banking', regex=False) \\\n",
    "                         .str.replace('t he', 'the', regex=False) \\\n",
    "                         .str.replace('fo llow', 'follow', regex=False) \\\n",
    "                         .str.replace('\\(  ', '(', regex=True) \\\n",
    "                         .str.replace('m aturity', 'maturity', regex=False) \\\n",
    "                         .str.replace('sy stems', 'systems', regex=False) \\\n",
    "                         .str.replace('Granularity ,', 'Granularity,', regex=False) \\\n",
    "                         .str.replace('interna l', 'internal', regex=False) \\\n",
    "                         .str.replace('data24', 'data 24', regex=False) \\\n",
    "                         .str.replace('co nsolidations', 'consolidations', regex=False) \\\n",
    "                         .str.replace('Relevan t', 'Relevant', regex=False) \\\n",
    "                         .str.replace('Regula tory', 'Regulatory', regex=False) \\\n",
    "                         .str.replace('Relev ant', 'Relevant', regex=False) \\\n",
    "                         .str.replace('applicat ion', 'application', regex=False) \\\n",
    "                         .str.replace('Quantificat ion', 'Quantification', regex=False) \\\n",
    "                         .str.replace('bac k-testing', 'back-testing', regex=False) \\\n",
    "                         .str.replace('adj ustments', 'adjustments', regex=False) \\\n",
    "                         .str.replace('inte rnal', 'internal', regex=False) \\\n",
    "                         .str.replace('rati ng', 'rating', regex=False) \\\n",
    "                         .str.replace('Remediatio n', 'Remediation', regex=False) \\\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621cb6a-e6ff-4896-a417-af1583559336",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trim_guide_pypdf_mod_v6_human_review.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "df = extract_data(text)\n",
    "columns_to_clean = ['Body', 'Level_0_Label', 'Level_1_Label', 'Level_2_Label']\n",
    "df = clean_text_columns(df, columns_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559dab4f-83bb-431b-bdac-c236b25552ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regular expression pattern for \"number space capital letter\"\n",
    "pattern = r'^\\d\\s[A-Z]'\n",
    "\n",
    "# Find all occurrences of numbers that follow a lowercase letter or a lowercase letter and a dot\n",
    "df['temp_extracted'] = df['Body'].str.findall(r'[a-z][\\.:,]?\\d+')\n",
    "\n",
    "# Remove the preceding lowercase letter and optional dot or colon, and convert to integers\n",
    "def clean_and_convert(lst):\n",
    "    new_lst = []\n",
    "    for item in lst:\n",
    "        number = re.search(r'\\d+', item)\n",
    "        if number:\n",
    "            new_lst.append(int(number.group()))\n",
    "    return new_lst\n",
    "\n",
    "df['footnotes'] = df['temp_extracted'].apply(clean_and_convert)\n",
    "df['footnote_flag'] = ~df['Level_5_Label'].isna()\n",
    "\n",
    "# Drop the temporary column\n",
    "df.drop(columns=['temp_extracted'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89041a15-1ed0-4f8d-ac30-147d8090d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "# Setting new index to start at 1\n",
    "df2.index = range(1, len(df) + 1)\n",
    "\n",
    "# Variable to keep track of the previous row\n",
    "previous_text = None\n",
    "previous_index = None\n",
    "\n",
    "# Extract footnotes\n",
    "condition = (df2['footnote_flag'] == 1)\n",
    "footnote_df = df2[condition].copy()\n",
    "df2.drop(df2.index[condition], inplace=True)\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "rows_list = []  # To store DataFrame pieces for concatenation\n",
    "\n",
    "# Loop through the rows\n",
    "for index, row in df2.iterrows():\n",
    "    # print(5, index, previous_index)\n",
    "\n",
    "    curr_concat = ''.join(map(str, row[['Level_1_Label', 'Level_2_Label', 'Level_3_Label']]))\n",
    "    \n",
    "    if previous_text is not None:\n",
    "        # Check if the text in the current row starts without a number\n",
    "        if (not row['Body'][0].isdigit()) and (previous_index is not None and index - previous_index != 1) and (curr_concat == previous_concat):\n",
    "            concatenated_text = previous_text + row['Body']\n",
    "            merged_row = row.copy()\n",
    "            merged_row['Body'] = concatenated_text\n",
    "            rows_list.append(merged_row)\n",
    "           \n",
    "            # Skip storing this row as previous since it's already concatenated\n",
    "            previous_text = None\n",
    "            previous_row = None\n",
    "            previous_index = index\n",
    "            continue\n",
    "        else:\n",
    "            # If condition not met, add the previous row to the frames list\n",
    "            rows_list.append(previous_row)\n",
    "\n",
    "    # Store the current row as previous for the next iteration\n",
    "    previous_text = row['Body']\n",
    "    previous_row = row.copy()\n",
    "    previous_index = index\n",
    "    previous_concat = ''.join(map(str, row[['Level_1_Label', 'Level_2_Label', 'Level_3_Label']]))\n",
    "\n",
    "# Append the last row if it was not concatenated\n",
    "if previous_text is not None:\n",
    "    rows_list.append(row)\n",
    "\n",
    "# Concatenate all DataFrame pieces\n",
    "new_df = pd.DataFrame(rows_list)\n",
    "new_df = pd.concat([new_df, footnote_df])\n",
    "\n",
    "# Add a new column based on the condition\n",
    "new_df['no_fault_detected'] = new_df['Body'].apply(starts_with_integer_ends_with_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403ab7a-bc89-4307-a535-2b1ab33853c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full path\n",
    "new_df['full_label'] = new_df['Level_0_Label'].str.cat([\n",
    "                                                        df['Level_1_Label'], \n",
    "                                                        df['Level_2_Label'], \n",
    "                                                        df['Level_3_Label']\n",
    "                                                       ], \n",
    "                                                       sep=' > ', \n",
    "                                                       na_rep='').str.strip(' > ')\n",
    "\n",
    "#  Identify a non-digit character (\\D) followed by a digit character (\\d) (cycle5 >> cycle 5)\n",
    "new_df['Body'] = new_df['Body'].str.replace(r'(\\D)(\\d)', r'\\1 \\2', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7631cc-2298-4560-a9ce-a3514fa266dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdebb7d2-45ea-4f3b-a1d4-86f9d45ea77a",
   "metadata": {},
   "source": [
    "#### E. Correct the spelling errors introduced during conversion with ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6b959-d18b-4bda-8573-62b81bee920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_spellcheck(phrase, max_retries=20, timeout=120):\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You perform a spellcheck and return correct sentences. \"\\\n",
    "                     \"The ONLY focus is on words that have been incorrectly split during the pdf conversion. \"\\\n",
    "                     \"No introduction or other explanations are necessary, just return corrected version of the text.\"},\n",
    "                {\"role\": \"user\", \"content\": phrase},\n",
    "            ]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo-0613\",\n",
    "                messages=messages,\n",
    "                request_timeout=timeout,\n",
    "            )\n",
    "            response_message = response[\"choices\"][0][\"message\"]['content']\n",
    "            return response_message\n",
    "        \n",
    "        except openai.error.OpenAIError as e:\n",
    "            print(f\"An error occurred: {e}. Retrying...\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}. Retrying...\")\n",
    "        \n",
    "        retries += 1\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"Maximum retries reached. Exiting...\")\n",
    "    return None\n",
    "\n",
    "# Initialize or reload a DataFrame column for checked sentences\n",
    "if \"checked_sentence\" not in new_df.columns:\n",
    "    new_df[\"checked_sentence\"] = \"\"\n",
    "\n",
    "# Start from the last successful index\n",
    "last_successful_index = new_df[new_df['checked_sentence'] != \"\"].index.max()\n",
    "start_index = 0 if pd.isna(last_successful_index) else last_successful_index + 1\n",
    "\n",
    "# Apply the function to the DataFrame starting from the last successful index\n",
    "for i, row in tqdm(new_df.iloc[start_index:].iterrows(), total=new_df.iloc[start_index:].shape[0]):\n",
    "    checked_sentence = gpt_spellcheck(row[\"Body\"])\n",
    "    \n",
    "    if checked_sentence is not None:\n",
    "        new_df.loc[i, \"checked_sentence\"] = checked_sentence\n",
    "        # Optionally, save the DataFrame at each step to keep the successful results\n",
    "        new_df.to_excel('gpt3_spellcheck_v2_checked.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a7b92-8b7b-4608-b071-9b1176c0ec8a",
   "metadata": {},
   "source": [
    "#### F. Create embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bc282-d9c5-4a94-9135-58daefdf5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee0e7c-ce47-4aa9-b679-c1f750e70216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_openai(phrase, max_retries=20, timeout=120):\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            response_message = get_embedding(phrase, engine=embedding_model)\n",
    "            return response_message\n",
    "        \n",
    "        except openai.error.OpenAIError as e:\n",
    "            print(f\"An error occurred: {e}. Retrying...\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}. Retrying...\")\n",
    "        \n",
    "        retries += 1\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"Maximum retries reached. Exiting...\")\n",
    "    return None\n",
    "\n",
    "\n",
    "df[\"combined\"] = (\"Source: \" + df['full_label'] + \"; Content: \" + df['checked_sentence'].str.strip())\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "# Initialize or reload a DataFrame column for checked sentences\n",
    "if \"embedding\" not in new_df.columns:\n",
    "    new_df[\"embedding\"] = \"\"\n",
    "\n",
    "# Start from the last successful index\n",
    "last_successful_index = new_df[new_df['embedding'] != \"\"].index.max()\n",
    "start_index = 0 if pd.isna(last_successful_index) else last_successful_index + 1\n",
    "\n",
    "# Apply the function to the DataFrame starting from the last successful index\n",
    "for i, row in tqdm(new_df.iloc[start_index:].iterrows(), total=new_df.iloc[start_index:].shape[0]):\n",
    "    embedding_result = get_embedding_openai(row[\"combined\"])\n",
    "    \n",
    "    if checked_sentence is not None:\n",
    "        new_df.loc[i, \"embedding\"] = str(embedding_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00acac2-d6c7-41a8-80f4-6e5f86ffdf0f",
   "metadata": {},
   "source": [
    "#### G. Final after-processing and saving of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c12dc-beaf-4f82-b510-bfff0637c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add index as the first column\n",
    "new_df = new_df.reset_index()\n",
    "new_df.rename(columns={'index': 'Index'}, inplace=True)\n",
    "\n",
    "# Create a pickle with the final results\n",
    "new_df.to_pickle(\"ecb_guide_embeddings.pkl\")\n",
    "new_df.to_excel(\"ecb_guide_embeddings.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f5925-845c-40d0-bc10-0c7741f3cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
